# -*- coding: utf-8 -*-
"""Output management for analysis results, reports, and file exports."""

import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple, Union
from datetime import datetime
import json


def to_array(x: Any) -> np.ndarray:
    """
    Convert various data types to numpy array.
    
    Parameters
    ----------
    x : Any
        Input data (pandas Series/DataFrame, list, or numpy array)
    
    Returns
    -------
    np.ndarray
        Converted numpy array
    """
    if x is None:
        return np.array([])
    if hasattr(x, 'values'):
        return x.values
    return np.array(x) if not isinstance(x, np.ndarray) else x


class OutputManager:
    """
    Professional output manager for ML-MCDM analysis results.
    
    Organizes outputs into a clean, professional structure:
    - results/         : All numerical results (CSV, JSON)
    - figures/         : All visualization charts (PNG)
    - reports/         : Comprehensive analysis reports
    """
    
    def __init__(self, base_output_dir: str = 'outputs'):
        """
        Initialize output manager.
        
        Parameters
        ----------
        base_output_dir : str
            Base directory for all outputs
        """
        self.base_dir = Path(base_output_dir)
        self.results_dir = self.base_dir / 'results'
        self.figures_dir = self.base_dir / 'figures'
        self.reports_dir = self.base_dir / 'reports'
        
        # Create directory structure
        self._setup_directories()
        
        self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    def _setup_directories(self) -> None:
        """Create clean directory structure."""
        # Create main directories
        for d in [self.results_dir, self.figures_dir, self.reports_dir]:
            d.mkdir(parents=True, exist_ok=True)
    
    # =========================================================================
    # NUMERICAL RESULTS EXPORT
    # =========================================================================
    
    def save_weights(self, weights: Dict[str, np.ndarray], 
                     component_names: List[str]) -> str:
        """
        Save all weight calculation results.
        
        Parameters
        ----------
        weights : Dict[str, np.ndarray]
            Dictionary of weight arrays by method
        component_names : List[str]
            Names of components/criteria
        
        Returns
        -------
        str
            Path to saved file
        """
        # Create comprehensive weights DataFrame
        df = pd.DataFrame({'Component': component_names})
        
        for method, w in weights.items():
            df[f'{method.title()}_Weight'] = w
        
        # Add statistics
        df['Mean_Weight'] = df[[c for c in df.columns if 'Weight' in c]].mean(axis=1)
        df['Std_Weight'] = df[[c for c in df.columns if 'Weight' in c]].std(axis=1)
        df['Rank'] = df['Mean_Weight'].rank(ascending=False).astype(int)
        
        # Sort by mean weight
        df = df.sort_values('Mean_Weight', ascending=False).reset_index(drop=True)
        
        save_path = self.results_dir / 'weights_analysis.csv'
        df.to_csv(save_path, index=False, float_format='%.6f')
        
        return str(save_path)
    
    def save_rankings(self, panel_data: Any, 
                      mcdm_results: Dict[str, Any],
                      ensemble_results: Dict[str, Any]) -> str:
        """
        Save comprehensive ranking results from all 11 MCDM methods.
        
        Parameters
        ----------
        panel_data : PanelData
            Panel data object
        mcdm_results : Dict
            MCDM analysis results
        ensemble_results : Dict
            Ensemble aggregation results
        
        Returns
        -------
        str
            Path to saved file
        """
        entities = panel_data.entities
        
        # Build comprehensive ranking DataFrame with all methods
        df = pd.DataFrame({
            'Entity': entities,
            # Traditional MCDM Rankings
            'TOPSIS_Score': to_array(mcdm_results['topsis_scores']),
            'TOPSIS_Rank': to_array(mcdm_results['topsis_rankings']),
            'Dynamic_TOPSIS_Score': to_array(mcdm_results['dynamic_topsis_scores']),
            'VIKOR_Q': to_array(mcdm_results['vikor']['Q']),
            'VIKOR_Rank': to_array(mcdm_results['vikor']['rankings']),
            'PROMETHEE_Phi_Net': to_array(mcdm_results['promethee']['phi_net']),
            'PROMETHEE_Rank': to_array(mcdm_results['promethee']['rankings']),
            'COPRAS_Utility': to_array(mcdm_results['copras']['utility_degree']),
            'COPRAS_Rank': to_array(mcdm_results['copras']['rankings']),
            'EDAS_AS': to_array(mcdm_results['edas']['AS']),
            'EDAS_Rank': to_array(mcdm_results['edas']['rankings']),
            # Fuzzy MCDM Rankings
            'Fuzzy_TOPSIS_Score': to_array(mcdm_results['fuzzy_topsis']['scores']),
            'Fuzzy_TOPSIS_Rank': to_array(mcdm_results['fuzzy_topsis']['rankings']),
            'Fuzzy_VIKOR_Q': to_array(mcdm_results['fuzzy_vikor']['Q']),
            'Fuzzy_VIKOR_Rank': to_array(mcdm_results['fuzzy_vikor']['rankings']),
            'Fuzzy_PROMETHEE_Phi_Net': to_array(mcdm_results['fuzzy_promethee']['phi_net']),
            'Fuzzy_PROMETHEE_Rank': to_array(mcdm_results['fuzzy_promethee']['rankings']),
            'Fuzzy_COPRAS_Utility': to_array(mcdm_results['fuzzy_copras']['utility_degree']),
            'Fuzzy_COPRAS_Rank': to_array(mcdm_results['fuzzy_copras']['rankings']),
            'Fuzzy_EDAS_AS': to_array(mcdm_results['fuzzy_edas']['AS']),
            'Fuzzy_EDAS_Rank': to_array(mcdm_results['fuzzy_edas']['rankings']),
        })
        
        # Add ensemble aggregated results
        if ensemble_results.get('aggregated'):
            agg = ensemble_results['aggregated']
            df['Final_Score'] = to_array(agg.final_scores)
            df['Final_Rank'] = to_array(agg.final_ranking)
            df['Kendall_W'] = agg.kendall_w
        
        # Sort by final rank or TOPSIS rank
        sort_col = 'Final_Rank' if 'Final_Rank' in df.columns else 'TOPSIS_Rank'
        df = df.sort_values(sort_col).reset_index(drop=True)
        df.insert(0, 'Rank', range(1, len(df) + 1))
        
        save_path = self.results_dir / 'final_rankings.csv'
        df.to_csv(save_path, index=False, float_format='%.6f')
        
        return str(save_path)
    
    def save_mcdm_scores(self, panel_data: Any,
                         mcdm_results: Dict[str, Any]) -> str:
        """
        Save detailed MCDM scores for all 11 methods.
        
        Includes: TOPSIS, Dynamic TOPSIS, VIKOR, PROMETHEE, COPRAS, EDAS,
                  Fuzzy TOPSIS, Fuzzy VIKOR, Fuzzy PROMETHEE, Fuzzy COPRAS, Fuzzy EDAS
        
        Parameters
        ----------
        panel_data : PanelData
            Panel data object
        mcdm_results : Dict
            MCDM analysis results
        
        Returns
        -------
        str
            Path to saved file
        """
        entities = panel_data.entities
        
        # Get TOPSIS distances from result object
        topsis_result = mcdm_results.get('topsis_result')
        d_positive = to_array(topsis_result.d_positive) if topsis_result and hasattr(topsis_result, 'd_positive') else np.full(len(entities), np.nan)
        d_negative = to_array(topsis_result.d_negative) if topsis_result and hasattr(topsis_result, 'd_negative') else np.full(len(entities), np.nan)
        
        # Build comprehensive DataFrame with all 11 MCDM methods
        df = pd.DataFrame({
            'Entity': entities,
            # Traditional MCDM Methods
            'TOPSIS_Score': to_array(mcdm_results['topsis_scores']),
            'TOPSIS_Rank': to_array(mcdm_results['topsis_rankings']),
            'TOPSIS_Distance_Positive': d_positive,
            'TOPSIS_Distance_Negative': d_negative,
            'Dynamic_TOPSIS_Score': to_array(mcdm_results['dynamic_topsis_scores']),
            'VIKOR_Q': to_array(mcdm_results['vikor']['Q']),
            'VIKOR_S': to_array(mcdm_results['vikor']['S']),
            'VIKOR_R': to_array(mcdm_results['vikor']['R']),
            'VIKOR_Rank': to_array(mcdm_results['vikor']['rankings']),
            'PROMETHEE_Phi_Net': to_array(mcdm_results['promethee']['phi_net']),
            'PROMETHEE_Phi_Positive': to_array(mcdm_results['promethee']['phi_positive']),
            'PROMETHEE_Phi_Negative': to_array(mcdm_results['promethee']['phi_negative']),
            'PROMETHEE_Rank': to_array(mcdm_results['promethee']['rankings']),
            'COPRAS_Utility': to_array(mcdm_results['copras']['utility_degree']),
            'COPRAS_Q': to_array(mcdm_results['copras']['Q']),
            'COPRAS_Rank': to_array(mcdm_results['copras']['rankings']),
            'EDAS_AS': to_array(mcdm_results['edas']['AS']),
            'EDAS_SP': to_array(mcdm_results['edas']['SP']),
            'EDAS_SN': to_array(mcdm_results['edas']['SN']),
            'EDAS_Rank': to_array(mcdm_results['edas']['rankings']),
            # Fuzzy MCDM Methods
            'Fuzzy_TOPSIS_Score': to_array(mcdm_results['fuzzy_topsis']['scores']),
            'Fuzzy_TOPSIS_Rank': to_array(mcdm_results['fuzzy_topsis']['rankings']),
            'Fuzzy_VIKOR_Q': to_array(mcdm_results['fuzzy_vikor']['Q']),
            'Fuzzy_VIKOR_S': to_array(mcdm_results['fuzzy_vikor']['S']),
            'Fuzzy_VIKOR_R': to_array(mcdm_results['fuzzy_vikor']['R']),
            'Fuzzy_VIKOR_Rank': to_array(mcdm_results['fuzzy_vikor']['rankings']),
            'Fuzzy_PROMETHEE_Phi_Net': to_array(mcdm_results['fuzzy_promethee']['phi_net']),
            'Fuzzy_PROMETHEE_Rank': to_array(mcdm_results['fuzzy_promethee']['rankings']),
            'Fuzzy_COPRAS_Utility': to_array(mcdm_results['fuzzy_copras']['utility_degree']),
            'Fuzzy_COPRAS_Q': to_array(mcdm_results['fuzzy_copras']['Q']),
            'Fuzzy_COPRAS_Rank': to_array(mcdm_results['fuzzy_copras']['rankings']),
            'Fuzzy_EDAS_AS': to_array(mcdm_results['fuzzy_edas']['AS']),
            'Fuzzy_EDAS_Rank': to_array(mcdm_results['fuzzy_edas']['rankings']),
        })
        
        # Sort by TOPSIS score (primary ranking criterion)
        df = df.sort_values('TOPSIS_Score', ascending=False).reset_index(drop=True)
        
        save_path = self.results_dir / 'mcdm_scores_detailed.csv'
        df.to_csv(save_path, index=False, float_format='%.6f')
        
        return str(save_path)
    
    def save_mcdm_rank_comparison(self, panel_data: Any,
                                   mcdm_results: Dict[str, Any]) -> str:
        """
        Save a comparison matrix of rankings from all 11 MCDM methods.
        
        This provides a clear side-by-side comparison of how each method
        ranks the alternatives, useful for analyzing method agreement.
        
        Parameters
        ----------
        panel_data : PanelData
            Panel data object
        mcdm_results : Dict
            MCDM analysis results
        
        Returns
        -------
        str
            Path to saved file
        """
        entities = panel_data.entities
        
        # Build ranking comparison DataFrame
        df = pd.DataFrame({
            'Entity': entities,
            'TOPSIS': to_array(mcdm_results['topsis_rankings']),
            'Dynamic_TOPSIS': len(entities) - np.argsort(np.argsort(to_array(mcdm_results['dynamic_topsis_scores']))),
            'VIKOR': to_array(mcdm_results['vikor']['rankings']),
            'PROMETHEE': to_array(mcdm_results['promethee']['rankings']),
            'COPRAS': to_array(mcdm_results['copras']['rankings']),
            'EDAS': to_array(mcdm_results['edas']['rankings']),
            'Fuzzy_TOPSIS': to_array(mcdm_results['fuzzy_topsis']['rankings']),
            'Fuzzy_VIKOR': to_array(mcdm_results['fuzzy_vikor']['rankings']),
            'Fuzzy_PROMETHEE': to_array(mcdm_results['fuzzy_promethee']['rankings']),
            'Fuzzy_COPRAS': to_array(mcdm_results['fuzzy_copras']['rankings']),
            'Fuzzy_EDAS': to_array(mcdm_results['fuzzy_edas']['rankings']),
        })
        
        # Add mean rank and rank standard deviation across all methods
        rank_cols = [c for c in df.columns if c != 'Entity']
        df['Mean_Rank'] = df[rank_cols].mean(axis=1)
        df['Rank_StdDev'] = df[rank_cols].std(axis=1)
        df['Rank_Range'] = df[rank_cols].max(axis=1) - df[rank_cols].min(axis=1)
        
        # Sort by mean rank
        df = df.sort_values('Mean_Rank').reset_index(drop=True)
        df.insert(0, 'Consensus_Rank', range(1, len(df) + 1))
        
        save_path = self.results_dir / 'mcdm_rank_comparison.csv'
        df.to_csv(save_path, index=False, float_format='%.2f')
        
        return str(save_path)
    
    def save_ml_results(self, ml_results: Dict[str, Any],
                        panel_data: Any) -> Dict[str, str]:
        """
        Save all ML analysis results.
        
        Parameters
        ----------
        ml_results : Dict
            ML analysis results
        panel_data : PanelData
            Panel data object
        
        Returns
        -------
        Dict[str, str]
            Paths to saved files
        """
        saved_files = {}
        
        # 1. Feature Importance
        if ml_results.get('rf_importance'):
            imp_df = pd.DataFrame([
                {'Feature': k, 'Importance': v, 
                 'Rank': i + 1}
                for i, (k, v) in enumerate(
                    sorted(ml_results['rf_importance'].items(), 
                           key=lambda x: x[1], reverse=True)
                )
            ])
            path = self.results_dir / 'feature_importance.csv'
            imp_df.to_csv(path, index=False, float_format='%.6f')
            saved_files['feature_importance'] = str(path)
        
        # 2. Cross-Validation Results
        if ml_results.get('rf_result'):
            rf = ml_results['rf_result']
            cv_df = pd.DataFrame(rf.cv_scores)
            cv_df['Fold'] = range(1, len(cv_df) + 1)
            cv_df = cv_df[['Fold'] + [c for c in cv_df.columns if c != 'Fold']]
            
            # Add summary statistics
            summary_row = {'Fold': 'Mean'}
            summary_row.update({k: np.mean(v) for k, v in rf.cv_scores.items()})
            cv_df = pd.concat([cv_df, pd.DataFrame([summary_row])], ignore_index=True)
            
            summary_row = {'Fold': 'Std'}
            summary_row.update({k: np.std(v) for k, v in rf.cv_scores.items()})
            cv_df = pd.concat([cv_df, pd.DataFrame([summary_row])], ignore_index=True)
            
            path = self.results_dir / 'cv_scores.csv'
            cv_df.to_csv(path, index=False, float_format='%.6f')
            saved_files['cv_scores'] = str(path)
            
            # Test metrics
            test_df = pd.DataFrame([rf.test_metrics])
            path = self.results_dir / 'rf_test_metrics.csv'
            test_df.to_csv(path, index=False, float_format='%.6f')
            saved_files['rf_test_metrics'] = str(path)
        
        # 3. Panel Regression Results
        if ml_results.get('panel_regression'):
            pr = ml_results['panel_regression']
            if hasattr(pr, 'coefficients'):
                coef_df = pd.DataFrame([
                    {'Variable': k, 'Coefficient': v}
                    for k, v in pr.coefficients.items()
                ])
                path = self.results_dir / 'panel_regression_coefficients.csv'
                coef_df.to_csv(path, index=False, float_format='%.6f')
                saved_files['panel_regression'] = str(path)
        
        return saved_files
    
    def save_ensemble_results(self, ensemble_results: Dict[str, Any],
                              panel_data: Any) -> Dict[str, str]:
        """
        Save ensemble model results.
        
        Parameters
        ----------
        ensemble_results : Dict
            Ensemble analysis results
        panel_data : PanelData
            Panel data object
        
        Returns
        -------
        Dict[str, str]
            Paths to saved files
        """
        saved_files = {}
        
        # 1. Stacking Results
        if ensemble_results.get('stacking'):
            stacking = ensemble_results['stacking']
            
            # Model weights
            weights_df = pd.DataFrame({
                'Base_Model': list(stacking.base_model_predictions.keys()),
                'Weight': stacking.meta_model_weights
            })
            weights_df = weights_df.sort_values('Weight', ascending=False)
            path = self.results_dir / 'stacking_weights.csv'
            weights_df.to_csv(path, index=False, float_format='%.6f')
            saved_files['stacking_weights'] = str(path)
            
            # Performance metrics
            metrics_df = pd.DataFrame([{
                'Meta_Model_R2': stacking.meta_model_r2,
                'Meta_Model_Type': type(stacking.meta_model).__name__ 
                    if hasattr(stacking, 'meta_model') else 'Unknown'
            }])
            path = self.results_dir / 'stacking_performance.csv'
            metrics_df.to_csv(path, index=False, float_format='%.6f')
            saved_files['stacking_performance'] = str(path)
        
        # 2. Rank Aggregation Metadata (save as JSON to avoid redundancy with final_rankings.csv)
        if ensemble_results.get('aggregated'):
            agg = ensemble_results['aggregated']
            
            # Save aggregation metadata as JSON (Kendall's W and method info)
            agg_metadata = {
                'kendall_w': float(agg.kendall_w),
                'interpretation': 'Strong agreement' if agg.kendall_w > 0.7 else 
                                 'Moderate agreement' if agg.kendall_w > 0.5 else 'Weak agreement',
                'n_entities': len(panel_data.entities),
                'aggregation_method': agg.method if hasattr(agg, 'method') else 'borda',
                'top_entity': panel_data.entities[int(np.argmin(to_array(agg.final_ranking)))],
                'top_score': float(np.max(to_array(agg.final_scores)))
            }
            
            path = self.results_dir / 'aggregation_metadata.json'
            with open(path, 'w') as f:
                json.dump(agg_metadata, f, indent=2)
            saved_files['aggregation_metadata'] = str(path)
        
        return saved_files
    
    def save_analysis_results(self, analysis_results: Dict[str, Any]) -> Dict[str, str]:
        """
        Save advanced analysis results.
        
        Parameters
        ----------
        analysis_results : Dict
            Analysis results (convergence, sensitivity)
        
        Returns
        -------
        Dict[str, str]
            Paths to saved files
        """
        saved_files = {}
        
        # 1. Convergence Analysis
        if analysis_results.get('convergence'):
            conv = analysis_results['convergence']
            
            # Sigma convergence by year
            sigma_df = pd.DataFrame({
                'Year': list(conv.sigma_by_year.keys()),
                'Coefficient_of_Variation': list(conv.sigma_by_year.values())
            })
            path = self.results_dir / 'sigma_convergence.csv'
            sigma_df.to_csv(path, index=False, float_format='%.6f')
            saved_files['sigma_convergence'] = str(path)
            
            # Beta convergence summary
            beta_df = pd.DataFrame([{
                'Beta_Coefficient': conv.beta_coefficient,
                'Half_Life_Years': conv.half_life,
                'Convergence_Type': 'Converging' if conv.beta_coefficient < 0 else 'Diverging',
                'Speed': abs(conv.beta_coefficient)
            }])
            path = self.results_dir / 'beta_convergence.csv'
            beta_df.to_csv(path, index=False, float_format='%.6f')
            saved_files['beta_convergence'] = str(path)
        
        # 2. Sensitivity Analysis
        if analysis_results.get('sensitivity'):
            sens = analysis_results['sensitivity']
            
            sens_df = pd.DataFrame([
                {'Criterion': k, 'Sensitivity_Index': v, 
                 'Rank': i + 1}
                for i, (k, v) in enumerate(
                    sorted(sens.weight_sensitivity.items(), 
                           key=lambda x: x[1], reverse=True)
                )
            ])
            path = self.results_dir / 'sensitivity_analysis.csv'
            sens_df.to_csv(path, index=False, float_format='%.6f')
            saved_files['sensitivity'] = str(path)
            
            # Overall robustness
            robust_df = pd.DataFrame([{
                'Overall_Robustness': sens.overall_robustness,
                'N_Simulations': sens.n_simulations if hasattr(sens, 'n_simulations') else 'N/A'
            }])
            path = self.results_dir / 'robustness_summary.csv'
            robust_df.to_csv(path, index=False, float_format='%.6f')
            saved_files['robustness'] = str(path)
        
        return saved_files
    
    def save_panel_data_summary(self, panel_data: Any) -> str:
        """
        Save panel data summary statistics.
        
        Parameters
        ----------
        panel_data : PanelData
            Panel data object
        
        Returns
        -------
        str
            Path to saved file
        """
        # Get latest cross-section
        latest = panel_data.get_latest()
        
        # Summary statistics
        summary = latest[panel_data.components].describe()
        summary = summary.T
        summary['Component'] = summary.index
        summary = summary[['Component'] + [c for c in summary.columns if c != 'Component']]
        
        path = self.results_dir / 'data_summary_statistics.csv'
        summary.to_csv(path, index=False, float_format='%.6f')
        
        return str(path)
    
    # =========================================================================
    # COMPREHENSIVE REPORT GENERATION
    # =========================================================================
    
    def generate_comprehensive_report(self, 
                                       panel_data: Any,
                                       weights: Dict[str, np.ndarray],
                                       mcdm_results: Dict[str, Any],
                                       ml_results: Dict[str, Any],
                                       ensemble_results: Dict[str, Any],
                                       analysis_results: Dict[str, Any],
                                       execution_time: float,
                                       future_predictions: Optional[Dict[str, Any]] = None) -> str:
        """
        Generate a comprehensive, publish-ready analysis report.
        
        Parameters
        ----------
        All analysis results and metadata
        
        Returns
        -------
        str
            Path to saved report
        """
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        current_year = max(panel_data.years)
        prediction_year = current_year + 1 if future_predictions else None
        
        # Get key statistics
        n_entities = len(panel_data.entities)
        n_years = len(panel_data.time_periods)
        n_components = len(panel_data.components)
        
        # Build comprehensive report
        report = []
        
        # =====================================================================
        # TITLE AND ABSTRACT
        # =====================================================================
        report.append("=" * 100)
        report.append("")
        report.append("            MULTI-CRITERIA DECISION MAKING ANALYSIS WITH MACHINE LEARNING")
        report.append("                    A Comprehensive Evaluation of Regional Sustainability")
        report.append("")
        report.append("=" * 100)
        report.append("")
        report.append(f"Report Generated: {timestamp}")
        report.append(f"Analysis Period: {min(panel_data.years)} - {max(panel_data.years)}")
        report.append(f"Computational Time: {execution_time:.2f} seconds")
        report.append("")
        report.append("-" * 100)
        report.append("EXECUTIVE SUMMARY")
        report.append("-" * 100)
        report.append("")
        
        # Executive Summary
        top_performer = None
        kendall_w = 0.0
        if ensemble_results.get('aggregated'):
            agg = ensemble_results['aggregated']
            final_ranking = to_array(agg.final_ranking)
            final_scores = to_array(agg.final_scores)
            best_idx = np.argmin(final_ranking)
            top_performer = panel_data.entities[best_idx]
            kendall_w = agg.kendall_w
        
        rf_r2 = ml_results['rf_result'].test_metrics.get('r2', 0) if ml_results.get('rf_result') else 0
        
        report.append(f"This report presents a comprehensive multi-criteria decision making (MCDM) analysis")
        report.append(f"of {n_entities} regional entities across {n_years} time periods ({min(panel_data.years)}-{max(panel_data.years)}),")
        report.append(f"evaluating performance based on {n_components} sustainability criteria.")
        report.append("")
        report.append("KEY FINDINGS:")
        report.append("")
        report.append(f"  1. TOP PERFORMER: {top_performer} consistently ranks as the leading entity across")
        report.append(f"     all MCDM methods, demonstrating superior performance in sustainability metrics.")
        report.append("")
        agreement_level = "excellent" if kendall_w > 0.8 else "strong" if kendall_w > 0.7 else "moderate" if kendall_w > 0.5 else "weak"
        report.append(f"  2. METHOD AGREEMENT: Kendall's W coefficient of {kendall_w:.4f} indicates {agreement_level}")
        report.append(f"     agreement among the 11 MCDM methods employed (6 traditional + 5 fuzzy),")
        report.append(f"     providing high confidence in the robustness of our rankings.")
        report.append("")
        report.append(f"  3. PREDICTIVE ACCURACY: Machine learning models achieve R² = {rf_r2:.4f}, demonstrating")
        report.append(f"     strong capability to explain and forecast sustainability performance patterns.")
        report.append("")
        
        if future_predictions:
            pred_scores = to_array(future_predictions['topsis_scores'])
            best_pred_idx = np.argmax(pred_scores)
            pred_top = panel_data.entities[best_pred_idx]
            report.append(f"  4. FORECAST: {pred_top} is predicted to maintain top performance in {prediction_year},")
            report.append(f"     with an expected TOPSIS score of {pred_scores[best_pred_idx]:.4f}.")
            report.append("")
        
        # =====================================================================
        # 1. INTRODUCTION AND METHODOLOGY
        # =====================================================================
        report.append("")
        report.append("=" * 100)
        report.append("1. INTRODUCTION AND METHODOLOGY")
        report.append("=" * 100)
        report.append("")
        report.append("1.1 Research Context")
        report.append("-" * 50)
        report.append("")
        report.append("Multi-criteria decision making (MCDM) provides a systematic framework for evaluating")
        report.append("alternatives against multiple, often conflicting criteria. This analysis employs an")
        report.append("integrated approach combining traditional MCDM methods, fuzzy extensions for handling")
        report.append("uncertainty, and machine learning for pattern recognition and forecasting.")
        report.append("")
        report.append("1.2 Dataset Description")
        report.append("-" * 50)
        report.append("")
        report.append(f"  • Entities Analyzed: {n_entities} regional units (provinces/regions)")
        report.append(f"  • Temporal Coverage: {n_years} years ({min(panel_data.years)}-{max(panel_data.years)})")
        report.append(f"  • Evaluation Criteria: {n_components} sustainability components")
        report.append(f"  • Total Observations: {n_entities * n_years} entity-year records")
        report.append("")
        report.append("1.3 Methodological Framework")
        report.append("-" * 50)
        report.append("")
        report.append("This analysis integrates multiple analytical approaches:")
        report.append("")
        report.append("  WEIGHTING METHODS:")
        report.append("    • Entropy Method: Derives weights from information content and variability")
        report.append("    • CRITIC Method: Incorporates both contrast intensity and inter-criteria correlation")
        report.append("    • PCA Method: Extracts weights from principal component loadings and explained variance")
        report.append("    • Ensemble Weights: Integrated hybrid of Entropy, CRITIC and PCA weights")
        report.append("")
        report.append("  MCDM METHODS (11 methods total):")
        report.append("    Traditional Methods:")
        report.append("      1. TOPSIS - Technique for Order Preference by Similarity to Ideal Solution")
        report.append("      2. Dynamic TOPSIS - Panel data extension with temporal dynamics")
        report.append("      3. VIKOR - Multi-criteria optimization and compromise solution")
        report.append("      4. PROMETHEE - Preference ranking with pairwise comparisons")
        report.append("      5. COPRAS - Complex proportional assessment of alternatives")
        report.append("      6. EDAS - Evaluation based on distance from average solution")
        report.append("    Fuzzy Extensions (handling temporal uncertainty):")
        report.append("      7. Fuzzy TOPSIS")
        report.append("      8. Fuzzy VIKOR")
        report.append("      9. Fuzzy PROMETHEE")
        report.append("      10. Fuzzy COPRAS")
        report.append("      11. Fuzzy EDAS")
        report.append("")
        report.append("  MACHINE LEARNING:")
        report.append("    • Random Forest with time-series cross-validation for feature importance")
        report.append("    • Unified Ensemble Forecasting (Gradient Boosting + Random Forest + Bayesian Ridge)")
        report.append("    • Future year prediction using all historical data")
        report.append("")
        report.append("  ENSEMBLE INTEGRATION:")
        report.append("    • Stacking ensemble with meta-learner optimization")
        report.append("    • Borda count rank aggregation for consensus ranking")
        report.append("")
        
        # =====================================================================
        # 2. CRITERIA WEIGHTING ANALYSIS
        # =====================================================================
        report.append("")
        report.append("=" * 100)
        report.append("2. CRITERIA WEIGHTING ANALYSIS")
        report.append("=" * 100)
        report.append("")
        report.append("Objective weighting methods were employed to determine the relative importance of each")
        report.append("criterion based on data characteristics, eliminating subjective bias in weight assignment.")
        report.append("")
        
        for method, w in weights.items():
            report.append(f"2.{list(weights.keys()).index(method)+1} {method.upper()} Weights")
            report.append("-" * 50)
            
            if method == 'entropy':
                report.append("The entropy method measures weight based on the amount of information conveyed by")
                report.append("each criterion. Criteria with greater variation carry more information and receive")
                report.append("higher weights.")
            elif method == 'critic':
                report.append("CRITIC (Criteria Importance Through Intercriteria Correlation) accounts for both")
                report.append("the contrast intensity (standard deviation) and conflicting relationships between")
                report.append("criteria to determine weights.")
            elif method == 'pca':
                report.append("PCA (Principal Component Analysis) derives weights from the variance structure of")
                report.append("the data. Weights reflect each criterion's contribution to the principal components,")
                report.append("weighted by explained variance ratios.")
            else:
                report.append("Ensemble weights combine Entropy, CRITIC and PCA methods through the integrated")
                report.append("hybrid strategy to leverage the strengths of each individual approach.")
            
            report.append("")
            report.append(f"  Statistical Summary:")
            report.append(f"    Minimum Weight: {w.min():.6f}")
            report.append(f"    Maximum Weight: {w.max():.6f}")
            report.append(f"    Mean Weight: {w.mean():.6f}")
            report.append(f"    Standard Deviation: {w.std():.6f}")
            report.append(f"    Weight Concentration (Max/Min ratio): {w.max()/w.min():.2f}x")
            report.append("")
            report.append("  Top 5 Most Important Criteria:")
            top_idx = np.argsort(w)[::-1][:5]
            for i, idx in enumerate(top_idx):
                pct = w[idx] / w.sum() * 100
                report.append(f"    {i+1}. {panel_data.components[idx]}: {w[idx]:.6f} ({pct:.1f}% of total)")
            report.append("")
        
        report.append("INTERPRETATION:")
        ensemble_w = weights['ensemble']
        top_3 = [panel_data.components[i] for i in np.argsort(ensemble_w)[::-1][:3]]
        report.append(f"The ensemble weighting identifies {', '.join(top_3)} as the most critical criteria")
        report.append("for sustainability assessment. These criteria exhibit both high variability and")
        report.append("significant discriminatory power, making them essential factors in distinguishing")
        report.append("high-performing entities from lower-performing ones.")
        report.append("")
        
        # =====================================================================
        # 3. MCDM ANALYSIS RESULTS
        # =====================================================================
        report.append("")
        report.append("=" * 100)
        report.append("3. MULTI-CRITERIA DECISION MAKING RESULTS")
        report.append("=" * 100)
        report.append("")
        
        # TOPSIS Analysis
        report.append("3.1 TOPSIS (Technique for Order Preference by Similarity to Ideal Solution)")
        report.append("-" * 50)
        report.append("")
        report.append("TOPSIS identifies the alternative that is simultaneously closest to the positive ideal")
        report.append("solution and farthest from the negative ideal solution. Scores range from 0 to 1,")
        report.append("where higher values indicate better overall performance.")
        report.append("")
        
        scores = to_array(mcdm_results['topsis_scores'])
        rankings = to_array(mcdm_results['topsis_rankings'])
        
        report.append("  Performance Distribution:")
        report.append(f"    Best Score: {scores.max():.6f}")
        report.append(f"    Worst Score: {scores.min():.6f}")
        report.append(f"    Mean Score: {scores.mean():.6f}")
        report.append(f"    Median Score: {np.median(scores):.6f}")
        report.append(f"    Standard Deviation: {scores.std():.6f}")
        report.append(f"    Coefficient of Variation: {scores.std()/scores.mean()*100:.1f}%")
        report.append("")
        
        report.append("  Top 10 Performing Entities:")
        report.append("  " + "-" * 70)
        report.append(f"  {'Rank':<6} {'Entity':<15} {'TOPSIS Score':<15} {'Performance Level'}")
        report.append("  " + "-" * 70)
        top_idx = np.argsort(rankings)[:10]
        for i, idx in enumerate(top_idx):
            perf_level = "Excellent" if scores[idx] > 0.7 else "Good" if scores[idx] > 0.5 else "Average"
            report.append(f"  {i+1:<6} {panel_data.entities[idx]:<15} {scores[idx]:<15.6f} {perf_level}")
        report.append("  " + "-" * 70)
        report.append("")
        
        # VIKOR Analysis
        report.append("3.2 VIKOR (Multi-Criteria Optimization and Compromise Solution)")
        report.append("-" * 50)
        report.append("")
        report.append("VIKOR focuses on ranking and selecting alternatives with conflicting criteria,")
        report.append("emphasizing compromise solutions. Lower Q values indicate better performance.")
        report.append("")
        
        vikor = mcdm_results['vikor']
        Q_vals = to_array(vikor['Q'])
        S_vals = to_array(vikor['S'])
        R_vals = to_array(vikor['R'])
        vikor_ranks = to_array(vikor['rankings'])
        
        report.append("  VIKOR Metrics Summary:")
        report.append(f"    Q (Compromise) Range: [{Q_vals.min():.6f}, {Q_vals.max():.6f}]")
        report.append(f"    S (Group Utility) Range: [{S_vals.min():.6f}, {S_vals.max():.6f}]")
        report.append(f"    R (Individual Regret) Range: [{R_vals.min():.6f}, {R_vals.max():.6f}]")
        report.append("")
        
        report.append("  Top 10 VIKOR Rankings (lowest Q = best):")
        report.append("  " + "-" * 80)
        report.append(f"  {'Rank':<6} {'Entity':<12} {'Q Value':<12} {'S Value':<12} {'R Value':<12} {'Status'}")
        report.append("  " + "-" * 80)
        top_vikor_idx = np.argsort(vikor_ranks)[:10]
        for i, idx in enumerate(top_vikor_idx):
            # Check acceptable advantage
            if i == 0:
                status = "Best Compromise"
            elif Q_vals[idx] - Q_vals[top_vikor_idx[0]] < 1/(n_entities-1):
                status = "Acceptable"
            else:
                status = "-"
            report.append(f"  {i+1:<6} {panel_data.entities[idx]:<12} {Q_vals[idx]:<12.6f} {S_vals[idx]:<12.6f} {R_vals[idx]:<12.6f} {status}")
        report.append("  " + "-" * 80)
        report.append("")
        
        # Fuzzy TOPSIS
        report.append("3.3 Fuzzy TOPSIS")
        report.append("-" * 50)
        report.append("")
        report.append("Fuzzy TOPSIS extends classical TOPSIS by representing criteria values as triangular")
        report.append("fuzzy numbers, capturing temporal variance and measurement uncertainty. This approach")
        report.append("provides more robust rankings under data imprecision.")
        report.append("")
        
        f_scores = to_array(mcdm_results['fuzzy_scores'])
        report.append(f"  Fuzzy Score Range: [{f_scores.min():.6f}, {f_scores.max():.6f}]")
        report.append(f"  Mean Fuzzy Score: {f_scores.mean():.6f}")
        report.append("")
        
        # Dynamic TOPSIS
        report.append("3.4 Dynamic TOPSIS (Panel-Aware Extension)")
        report.append("-" * 50)
        report.append("")
        report.append("Dynamic TOPSIS incorporates temporal dynamics by considering:")
        report.append("  • Trajectory analysis: Direction and rate of performance change over time")
        report.append("  • Stability weighting: Consistency of performance across periods")
        report.append("  • Temporal discounting: Greater emphasis on recent performance")
        report.append("")
        
        d_scores = to_array(mcdm_results['dynamic_topsis_scores'])
        report.append(f"  Dynamic Score Range: [{d_scores.min():.6f}, {d_scores.max():.6f}]")
        report.append(f"  Mean Dynamic Score: {d_scores.mean():.6f}")
        report.append("")
        
        # PROMETHEE Analysis
        report.append("3.5 PROMETHEE (Preference Ranking Organization Method)")
        report.append("-" * 50)
        report.append("")
        report.append("PROMETHEE employs pairwise comparison with preference functions to establish")
        report.append("dominance relationships. The net flow (Φ_net) represents overall preference,")
        report.append("where higher values indicate better performance.")
        report.append("")
        
        promethee = mcdm_results['promethee']
        phi_net = to_array(promethee['phi_net'])
        phi_pos = to_array(promethee['phi_positive'])
        phi_neg = to_array(promethee['phi_negative'])
        prom_ranks = to_array(promethee['rankings'])
        
        report.append("  PROMETHEE Flow Metrics:")
        report.append(f"    Φ_net (Net Flow) Range: [{phi_net.min():.6f}, {phi_net.max():.6f}]")
        report.append(f"    Φ+ (Positive Flow) Range: [{phi_pos.min():.6f}, {phi_pos.max():.6f}]")
        report.append(f"    Φ- (Negative Flow) Range: [{phi_neg.min():.6f}, {phi_neg.max():.6f}]")
        report.append("")
        
        report.append("  Top 5 PROMETHEE II Rankings:")
        top_prom = np.argsort(prom_ranks)[:5]
        for i, idx in enumerate(top_prom):
            report.append(f"    {i+1}. {panel_data.entities[idx]}: Φ_net = {phi_net[idx]:.6f}")
        report.append("")
        
        # COPRAS Analysis
        report.append("3.6 COPRAS (Complex Proportional Assessment)")
        report.append("-" * 50)
        report.append("")
        report.append("COPRAS evaluates alternatives by separately assessing beneficial and non-beneficial")
        report.append("criteria, with utility degree as the final performance measure (0-100%).")
        report.append("")
        
        copras = mcdm_results['copras']
        utility = to_array(copras['utility_degree'])
        copras_Q = to_array(copras['Q'])
        copras_ranks = to_array(copras['rankings'])
        
        report.append("  COPRAS Performance Metrics:")
        report.append(f"    Utility Degree Range: {utility.min():.2f}% - {utility.max():.2f}%")
        report.append(f"    Mean Utility: {utility.mean():.2f}%")
        report.append(f"    Q Index Range: [{copras_Q.min():.6f}, {copras_Q.max():.6f}]")
        report.append("")
        
        report.append("  Top 5 COPRAS Rankings:")
        top_copras = np.argsort(copras_ranks)[:5]
        for i, idx in enumerate(top_copras):
            report.append(f"    {i+1}. {panel_data.entities[idx]}: Utility = {utility[idx]:.2f}%")
        report.append("")
        
        # EDAS Analysis
        report.append("3.7 EDAS (Evaluation based on Distance from Average Solution)")
        report.append("-" * 50)
        report.append("")
        report.append("EDAS assesses alternatives based on distance from average solution (AV).")
        report.append("The appraisal score (AS) combines positive and negative distances, where")
        report.append("higher AS values indicate superior performance. Range: 0 to 1.")
        report.append("")
        
        edas = mcdm_results['edas']
        edas_AS = to_array(edas['AS'])
        edas_SP = to_array(edas['SP'])
        edas_SN = to_array(edas['SN'])
        edas_ranks = to_array(edas['rankings'])
        
        report.append("  EDAS Metrics:")
        report.append(f"    Appraisal Score (AS) Range: [{edas_AS.min():.6f}, {edas_AS.max():.6f}]")
        report.append(f"    Mean AS: {edas_AS.mean():.6f}")
        report.append(f"    Positive Distance (SP) Mean: {edas_SP.mean():.6f}")
        report.append(f"    Negative Distance (SN) Mean: {edas_SN.mean():.6f}")
        report.append("")
        
        report.append("  Top 5 EDAS Rankings:")
        top_edas = np.argsort(edas_ranks)[:5]
        for i, idx in enumerate(top_edas):
            report.append(f"    {i+1}. {panel_data.entities[idx]}: AS = {edas_AS[idx]:.6f}")
        report.append("")
        
        # Fuzzy Extensions Summary
        report.append("3.8 Fuzzy MCDM Extensions")
        report.append("-" * 50)
        report.append("")
        report.append("All five traditional methods (TOPSIS, VIKOR, PROMETHEE, COPRAS, EDAS) have")
        report.append("fuzzy counterparts that handle temporal uncertainty using triangular fuzzy")
        report.append("numbers derived from panel data variance.")
        report.append("")
        
        fuzzy_vikor = mcdm_results['fuzzy_vikor']
        fuzzy_prom = mcdm_results['fuzzy_promethee']
        fuzzy_copras = mcdm_results['fuzzy_copras']
        fuzzy_edas = mcdm_results['fuzzy_edas']
        
        report.append("  Fuzzy Method Performance Summary:")
        report.append(f"    Fuzzy VIKOR - Best Q: {to_array(fuzzy_vikor['Q']).min():.6f}")
        report.append(f"    Fuzzy PROMETHEE - Best Φ_net: {to_array(fuzzy_prom['phi_net']).max():.6f}")
        report.append(f"    Fuzzy COPRAS - Best Utility: {to_array(fuzzy_copras['utility_degree']).max():.2f}%")
        report.append(f"    Fuzzy EDAS - Best AS: {to_array(fuzzy_edas['AS']).max():.6f}")
        report.append("")
        
        # Method Agreement
        report.append("3.9 Cross-Method Validation and Agreement")
        report.append("-" * 50)
        report.append("")
        report.append("To validate ranking robustness, we compare results across all MCDM methods:")
        report.append("")
        
        # Calculate rank correlation between TOPSIS and VIKOR
        topsis_order = np.argsort(np.argsort(rankings))
        vikor_order = np.argsort(np.argsort(vikor_ranks))
        rank_corr = 1 - 6 * np.sum((topsis_order - vikor_order)**2) / (n_entities * (n_entities**2 - 1))
        
        report.append(f"  TOPSIS vs VIKOR Spearman Correlation: {rank_corr:.4f}")
        agreement = "excellent" if abs(rank_corr) > 0.9 else "strong" if abs(rank_corr) > 0.7 else "moderate"
        report.append(f"  Interpretation: {agreement.capitalize()} agreement between classical MCDM methods")
        report.append("")
        
        # =====================================================================
        # 4. MACHINE LEARNING ANALYSIS
        # =====================================================================
        report.append("")
        report.append("=" * 100)
        report.append("4. MACHINE LEARNING ANALYSIS")
        report.append("=" * 100)
        report.append("")
        
        if ml_results.get('rf_result'):
            rf = ml_results['rf_result']
            
            report.append("4.1 Random Forest Feature Importance Analysis")
            report.append("-" * 50)
            report.append("")
            report.append("A Random Forest regressor with time-series cross-validation was trained to predict")
            report.append("TOPSIS scores from criteria values. This analysis identifies which criteria most")
            report.append("strongly influence overall sustainability rankings.")
            report.append("")
            
            report.append("  Model Performance Metrics:")
            report.append(f"    Test R-squared: {rf.test_metrics.get('r2', 0):.6f}")
            report.append(f"    Test MAE: {rf.test_metrics.get('mae', 0):.6f}")
            report.append(f"    Test RMSE: {np.sqrt(rf.test_metrics.get('mse', 0)):.6f}")
            report.append(f"    Rank Correlation (Spearman): {rf.rank_correlation:.6f}")
            report.append("")
            
            report.append("  Cross-Validation Results (Time-Series Split):")
            for metric, values in rf.cv_scores.items():
                report.append(f"    {metric.upper()}: {np.mean(values):.6f} ± {np.std(values):.6f}")
            report.append("")
            
            report.append("  INTERPRETATION:")
            r2 = rf.test_metrics.get('r2', 0)
            if r2 > 0.9:
                interp = "excellent predictive power, explaining over 90% of variance"
            elif r2 > 0.7:
                interp = "strong predictive capability with high explanatory power"
            elif r2 > 0.5:
                interp = "moderate predictive ability with room for improvement"
            else:
                interp = "limited predictive power, suggesting non-linear relationships"
            report.append(f"  The model demonstrates {interp}.")
            report.append("")
            
            report.append("  Feature Importance Rankings:")
            report.append("  " + "-" * 60)
            report.append(f"  {'Rank':<6} {'Criterion':<15} {'Importance':<12} {'Cumulative %'}")
            report.append("  " + "-" * 60)
            
            sorted_imp = sorted(ml_results['rf_importance'].items(), key=lambda x: x[1], reverse=True)
            cumulative = 0
            for i, (feat, imp) in enumerate(sorted_imp[:15]):
                cumulative += imp
                report.append(f"  {i+1:<6} {feat:<15} {imp:<12.6f} {cumulative*100:.1f}%")
            report.append("  " + "-" * 60)
            report.append("")
            
            report.append("  KEY INSIGHT:")
            top_features = [f[0] for f in sorted_imp[:3]]
            report.append(f"  The top 3 criteria ({', '.join(top_features)}) account for")
            top_3_imp = sum([f[1] for f in sorted_imp[:3]])
            report.append(f"  {top_3_imp*100:.1f}% of the total feature importance, indicating these are the")
            report.append("  primary drivers of sustainability performance differentiation.")
            report.append("")
        
        # =====================================================================
        # 5. ENSEMBLE INTEGRATION
        # =====================================================================
        report.append("")
        report.append("=" * 100)
        report.append("5. ENSEMBLE INTEGRATION AND FINAL RANKINGS")
        report.append("=" * 100)
        report.append("")
        
        if ensemble_results.get('stacking'):
            stacking = ensemble_results['stacking']
            
            report.append("5.1 Stacking Ensemble Meta-Learner")
            report.append("-" * 50)
            report.append("")
            report.append("A stacking ensemble combines predictions from all base MCDM methods using a")
            report.append("regularized meta-learner to optimize the final score predictions.")
            report.append("")
            
            report.append(f"  Meta-Model Performance (R²): {stacking.meta_model_r2:.6f}")
            report.append("")
            
            report.append("  Base Model Contribution Weights:")
            report.append("  " + "-" * 50)
            sorted_weights = sorted(zip(stacking.base_model_predictions.keys(), 
                                       stacking.meta_model_weights), 
                                   key=lambda x: x[1], reverse=True)
            for model, weight in sorted_weights:
                bar = "█" * int(weight * 40)
                report.append(f"    {model:<20}: {weight:.4f} {bar}")
            report.append("  " + "-" * 50)
            report.append("")
        
        if ensemble_results.get('aggregated'):
            agg = ensemble_results['aggregated']
            
            report.append("5.2 Rank Aggregation (Borda Count)")
            report.append("-" * 50)
            report.append("")
            report.append("Borda count aggregation combines rankings from all MCDM methods by assigning")
            report.append("points based on rank position. This produces a consensus ranking that reflects")
            report.append("agreement across multiple methodological perspectives.")
            report.append("")
            
            report.append(f"  Kendall's W (Inter-Method Agreement): {agg.kendall_w:.6f}")
            
            if agg.kendall_w > 0.8:
                w_interp = "excellent agreement - rankings are highly consistent across methods"
            elif agg.kendall_w > 0.7:
                w_interp = "strong agreement - methods largely concur on rankings"
            elif agg.kendall_w > 0.5:
                w_interp = "moderate agreement - some divergence in method rankings"
            else:
                w_interp = "weak agreement - methods produce divergent rankings"
            report.append(f"  Interpretation: {w_interp}")
            report.append("")
            
            final_ranking = to_array(agg.final_ranking)
            final_scores = to_array(agg.final_scores)
            
            report.append("  FINAL CONSENSUS RANKINGS:")
            report.append("  " + "=" * 70)
            report.append(f"  {'Rank':<6} {'Entity':<15} {'Borda Score':<15} {'Percentile'}")
            report.append("  " + "=" * 70)
            
            sorted_idx = np.argsort(final_ranking)
            for i, idx in enumerate(sorted_idx):
                percentile = (1 - i/n_entities) * 100
                perf = "★★★ Top 10%" if percentile > 90 else "★★ Top 25%" if percentile > 75 else "★ Top 50%" if percentile > 50 else ""
                report.append(f"  {i+1:<6} {panel_data.entities[idx]:<15} {final_scores[idx]:<15.4f} {perf}")
            report.append("  " + "=" * 70)
            report.append("")
        
        # =====================================================================
        # 6. SENSITIVITY AND ROBUSTNESS ANALYSIS
        # =====================================================================
        report.append("")
        report.append("=" * 100)
        report.append("6. SENSITIVITY AND ROBUSTNESS ANALYSIS")
        report.append("=" * 100)
        report.append("")
        
        if analysis_results.get('sensitivity'):
            sens = analysis_results['sensitivity']
            
            report.append("6.1 Weight Perturbation Sensitivity")
            report.append("-" * 50)
            report.append("")
            report.append("Monte Carlo simulation with 1,000 random weight perturbations tests ranking")
            report.append("stability. High sensitivity indices indicate criteria whose weights strongly")
            report.append("influence final rankings.")
            report.append("")
            
            report.append(f"  Overall Robustness Score: {sens.overall_robustness:.4f}")
            
            if sens.overall_robustness > 0.9:
                rob_interp = "Rankings are highly stable - minor weight changes have minimal impact"
            elif sens.overall_robustness > 0.7:
                rob_interp = "Rankings are reasonably robust to weight perturbations"
            else:
                rob_interp = "Rankings show sensitivity to weight changes - interpret with caution"
            report.append(f"  Interpretation: {rob_interp}")
            report.append("")
            
            report.append("  Criteria Sensitivity Index (normalized 0-1):")
            report.append("  " + "-" * 60)
            sorted_sens = sorted(sens.weight_sensitivity.items(), key=lambda x: x[1], reverse=True)
            for i, (crit, val) in enumerate(sorted_sens[:10]):
                bar = "█" * int(val * 30)
                sens_level = "HIGH" if val > 0.6 else "MEDIUM" if val > 0.3 else "LOW"
                report.append(f"    {crit}: {val:.4f} {bar} [{sens_level}]")
            report.append("  " + "-" * 60)
            report.append("")
            
            report.append("  POLICY IMPLICATION:")
            high_sens = [c for c, v in sorted_sens if v > 0.5][:3]
            if high_sens:
                report.append(f"  Criteria {', '.join(high_sens)} have high sensitivity. Policy interventions")
                report.append("  targeting these areas will have the greatest impact on rankings.")
            report.append("")
        
        if analysis_results.get('convergence'):
            conv = analysis_results['convergence']
            
            report.append("6.2 Regional Convergence Analysis")
            report.append("-" * 50)
            report.append("")
            report.append("Convergence analysis examines whether regional disparities are narrowing (σ-convergence)")
            report.append("and whether initially disadvantaged regions are catching up (β-convergence).")
            report.append("")
            
            report.append(f"  Beta Coefficient: {conv.beta_coefficient:.6f}")
            report.append(f"  Estimated Half-Life: {conv.half_life:.1f} years")
            
            if conv.beta_coefficient < 0:
                report.append("  Status: CONVERGENCE DETECTED")
                report.append(f"  At current rates, regional gaps will halve every {conv.half_life:.1f} years.")
            else:
                report.append("  Status: DIVERGENCE DETECTED")
                report.append("  Regional inequalities are widening over time.")
            report.append("")
            
            report.append("  Sigma Convergence (Coefficient of Variation by Year):")
            for year, sigma in conv.sigma_by_year.items():
                trend = ""
                report.append(f"    {year}: {sigma:.6f} {trend}")
            report.append("")
        
        # =====================================================================
        # 7. FORECASTING RESULTS
        # =====================================================================
        if future_predictions:
            report.append("")
            report.append("=" * 100)
            report.append(f"7. FORECASTING RESULTS FOR {prediction_year}")
            report.append("=" * 100)
            report.append("")
            
            report.append("7.1 Methodology")
            report.append("-" * 50)
            report.append("")
            report.append(f"An ensemble of machine learning models was trained on {n_years} years of historical")
            report.append(f"data ({min(panel_data.years)}-{max(panel_data.years)}) to forecast criteria values for {prediction_year}.")
            report.append("The ensemble combines Gradient Boosting, Bayesian Ridge, and Huber regression")
            report.append("with automatic performance-based weighting.")
            report.append("")
            
            model_contrib = future_predictions.get('model_contributions', {})
            if model_contrib:
                report.append("  Model Contributions to Ensemble:")
                sorted_models = sorted(model_contrib.items(), key=lambda x: x[1], reverse=True)
                for model, weight in sorted_models[:5]:
                    bar = "█" * int(weight * 40)
                    report.append(f"    {model:<20}: {weight:.4f} {bar}")
                report.append("")
            
            report.append(f"7.2 Predicted Rankings for {prediction_year}")
            report.append("-" * 50)
            report.append("")
            
            pred_scores = to_array(future_predictions['topsis_scores'])
            pred_ranks = to_array(future_predictions['topsis_rankings'])
            
            report.append("  Predicted TOPSIS Score Distribution:")
            report.append(f"    Best Predicted Score: {pred_scores.max():.6f}")
            report.append(f"    Worst Predicted Score: {pred_scores.min():.6f}")
            report.append(f"    Mean Predicted Score: {pred_scores.mean():.6f}")
            report.append(f"    Score Change from {current_year}: {pred_scores.mean() - scores.mean():+.6f}")
            report.append("")
            
            report.append(f"  Predicted Top 15 Rankings for {prediction_year}:")
            report.append("  " + "=" * 70)
            report.append(f"  {'Rank':<6} {'Entity':<15} {'Predicted Score':<18} {'Change from {}':<15}".format(current_year))
            report.append("  " + "=" * 70)
            
            top_pred_idx = np.argsort(pred_ranks)[:15]
            for i, idx in enumerate(top_pred_idx):
                current_rank = rankings[idx]
                rank_change = int(current_rank) - (i+1)
                change_str = f"↑{rank_change}" if rank_change > 0 else f"↓{abs(rank_change)}" if rank_change < 0 else "→"
                report.append(f"  {i+1:<6} {panel_data.entities[idx]:<15} {pred_scores[idx]:<18.6f} {change_str}")
            report.append("  " + "=" * 70)
            report.append("")
            
            if 'vikor' in future_predictions:
                vikor_pred = future_predictions['vikor']
                report.append(f"7.3 Predicted VIKOR Analysis for {prediction_year}")
                report.append("-" * 50)
                report.append("")
                
                Q_pred = to_array(vikor_pred['Q'])
                report.append(f"  Predicted Q Value Range: [{Q_pred.min():.6f}, {Q_pred.max():.6f}]")
                report.append("")
                
                report.append("  Predicted VIKOR Top 5:")
                vikor_pred_ranks = to_array(vikor_pred['rankings'])
                top_vikor_pred = np.argsort(vikor_pred_ranks)[:5]
                for i, idx in enumerate(top_vikor_pred):
                    report.append(f"    {i+1}. {panel_data.entities[idx]}: Q = {Q_pred[idx]:.6f}")
                report.append("")
        
        # =====================================================================
        # 8. CONCLUSIONS AND RECOMMENDATIONS
        # =====================================================================
        report.append("")
        report.append("=" * 100)
        report.append("8. CONCLUSIONS AND RECOMMENDATIONS" if future_predictions else "7. CONCLUSIONS AND RECOMMENDATIONS")
        report.append("=" * 100)
        report.append("")
        
        report.append("8.1 Summary of Key Findings")
        report.append("-" * 50)
        report.append("")
        
        # Top performer analysis
        if ensemble_results.get('aggregated'):
            agg = ensemble_results['aggregated']
            final_ranking = to_array(agg.final_ranking)
            final_scores = to_array(agg.final_scores)
            top_3_idx = np.argsort(final_ranking)[:3]
            bottom_3_idx = np.argsort(final_ranking)[-3:][::-1]
            
            report.append(f"  1. PERFORMANCE LEADERS ({current_year}):")
            for i, idx in enumerate(top_3_idx):
                report.append(f"     • {panel_data.entities[idx]} (Rank {i+1}): Demonstrates excellence across")
                report.append(f"       multiple sustainability dimensions with Borda score {final_scores[idx]:.2f}")
            report.append("")
            
            report.append(f"  2. AREAS FOR IMPROVEMENT:")
            for i, idx in enumerate(bottom_3_idx):
                report.append(f"     • {panel_data.entities[idx]} (Rank {n_entities-i}): Requires targeted")
                report.append(f"       interventions to improve sustainability performance")
            report.append("")
        
        report.append(f"  3. METHODOLOGICAL ROBUSTNESS:")
        report.append(f"     • {agreement_level.capitalize()} inter-method agreement (Kendall's W = {kendall_w:.4f})")
        report.append(f"     • ML model explains {rf_r2*100:.1f}% of performance variance")
        if analysis_results.get('sensitivity'):
            report.append(f"     • Rankings show {sens.overall_robustness*100:.1f}% robustness to weight perturbations")
        report.append("")
        
        if future_predictions:
            report.append(f"  4. FORECAST ({prediction_year}):")
            pred_scores = to_array(future_predictions['topsis_scores'])
            report.append(f"     • Mean predicted score: {pred_scores.mean():.4f}")
            trend = "improvement" if pred_scores.mean() > scores.mean() else "decline"
            report.append(f"     • Overall trend: Slight {trend} expected")
            report.append("")
        
        report.append("8.2 Policy Recommendations")
        report.append("-" * 50)
        report.append("")
        
        # Get top important features
        if ml_results.get('rf_importance'):
            top_criteria = [f[0] for f in sorted(ml_results['rf_importance'].items(), 
                                                  key=lambda x: x[1], reverse=True)[:3]]
            report.append(f"  1. PRIORITY FOCUS AREAS: {', '.join(top_criteria)}")
            report.append("     These criteria have the highest influence on sustainability rankings.")
            report.append("     Policy interventions should prioritize improvements in these areas.")
            report.append("")
        
        if analysis_results.get('sensitivity'):
            high_sens_criteria = [c for c, v in sorted(sens.weight_sensitivity.items(), 
                                                        key=lambda x: x[1], reverse=True)[:3]]
            report.append(f"  2. SENSITIVE INDICATORS: {', '.join(high_sens_criteria)}")
            report.append("     Rankings are most sensitive to these criteria weights.")
            report.append("     Ensure accurate measurement and appropriate weighting for these factors.")
            report.append("")
        
        if analysis_results.get('convergence') and conv.beta_coefficient < 0:
            report.append("  3. CONVERGENCE SUPPORT:")
            report.append(f"     Regional convergence is occurring (half-life: {conv.half_life:.1f} years).")
            report.append("     Continue policies supporting lagging regions to accelerate catch-up.")
        elif analysis_results.get('convergence'):
            report.append("  3. DIVERGENCE MITIGATION:")
            report.append("     Regional disparities are widening. Consider targeted support programs")
            report.append("     for underperforming entities to reverse this trend.")
        report.append("")
        
        report.append("8.3 Limitations and Future Research")
        report.append("-" * 50)
        report.append("")
        report.append(f"  • Data limited to {n_years} years; longer time series would improve forecasting accuracy")
        report.append("  • Equal treatment of all criteria types (benefit/cost) may oversimplify")
        report.append("  • External factors (policy changes, economic shocks) not explicitly modeled")
        report.append("  • Future research should incorporate spatial spillover effects")
        report.append("")
        
        # =====================================================================
        # APPENDIX: TECHNICAL DETAILS
        # =====================================================================
        report.append("")
        report.append("=" * 100)
        report.append("APPENDIX: TECHNICAL SPECIFICATIONS")
        report.append("=" * 100)
        report.append("")
        report.append("A.1 Computational Environment")
        report.append("-" * 50)
        report.append(f"  Total Execution Time: {execution_time:.2f} seconds")
        report.append(f"  Report Generated: {timestamp}")
        report.append("")
        report.append("A.2 MCDM Method Parameters")
        report.append("-" * 50)
        report.append("  TOPSIS: Vector normalization, ensemble weights")
        report.append("  Dynamic TOPSIS: Temporal discount=0.9, trajectory weight=0.3, stability weight=0.2")
        report.append("  VIKOR: v parameter=0.5 (group utility vs individual regret trade-off)")
        report.append("  Fuzzy: Triangular fuzzy numbers with temporal variance modeling")
        report.append("")
        report.append("A.3 Machine Learning Configuration")
        report.append("-" * 50)
        report.append("  Random Forest: 200 estimators, max_depth=10, time-series CV (2 splits)")
        report.append("  Ensemble Forecasting: Gradient Boosting + Bayesian Ridge + Huber")
        report.append("  Cross-validation: Time-series aware splitting (no data leakage)")
        report.append("")
        
        report.append("=" * 100)
        report.append("END OF REPORT")
        report.append("=" * 100)
        
        # Save report
        report_text = '\n'.join(report)
        path = self.reports_dir / 'report.txt'
        with open(path, 'w', encoding='utf-8') as f:
            f.write(report_text)
        
        return str(path)
    
    def save_future_predictions(self, future_predictions: Dict[str, Any],
                                panel_data: Any) -> Dict[str, str]:
        """
        Save predicted future year (2025) results.
        
        Parameters
        ----------
        future_predictions : Dict
            Future prediction results including predicted components and MCDM scores
        panel_data : PanelData
            Panel data object
        
        Returns
        -------
        Dict[str, str]
            Paths to saved files
        """
        saved_files = {}
        prediction_year = future_predictions.get('prediction_year', 2025)
        
        # 1. Save predicted rankings for future year
        entities = panel_data.entities
        
        rankings_df = pd.DataFrame({
            'Rank': range(1, len(entities) + 1),
            'Entity': entities,
            'Predicted_TOPSIS_Score': to_array(future_predictions['topsis_scores']),
            'Predicted_TOPSIS_Rank': to_array(future_predictions['topsis_rankings']),
            'Predicted_VIKOR_Q': to_array(future_predictions['vikor']['Q']),
            'Predicted_VIKOR_S': to_array(future_predictions['vikor']['S']),
            'Predicted_VIKOR_R': to_array(future_predictions['vikor']['R']),
            'Predicted_VIKOR_Rank': to_array(future_predictions['vikor']['rankings']),
            'Prediction_Year': prediction_year
        })
        
        # Sort by predicted TOPSIS rank
        rankings_df = rankings_df.sort_values('Predicted_TOPSIS_Rank').reset_index(drop=True)
        rankings_df['Rank'] = range(1, len(rankings_df) + 1)
        
        path = self.results_dir / f'predicted_rankings_{prediction_year}.csv'
        rankings_df.to_csv(path, index=False, float_format='%.6f')
        saved_files['predicted_rankings'] = str(path)
        
        # 2. Save predicted component values
        predicted_components = future_predictions.get('predicted_components')
        if predicted_components is not None:
            if isinstance(predicted_components, pd.DataFrame):
                comp_df = predicted_components.copy()
                comp_df.insert(0, 'Entity', comp_df.index)
                comp_df = comp_df.reset_index(drop=True)
            else:
                comp_df = pd.DataFrame(
                    predicted_components,
                    index=entities,
                    columns=panel_data.components
                )
                comp_df.insert(0, 'Entity', comp_df.index)
                comp_df = comp_df.reset_index(drop=True)
            
            comp_df['Prediction_Year'] = prediction_year
            
            path = self.results_dir / f'predicted_components_{prediction_year}.csv'
            comp_df.to_csv(path, index=False, float_format='%.6f')
            saved_files['predicted_components'] = str(path)
        
        # 3. Save prediction uncertainty if available
        uncertainty = future_predictions.get('prediction_uncertainty')
        if uncertainty is not None:
            if isinstance(uncertainty, pd.DataFrame):
                unc_df = uncertainty.copy()
                unc_df.insert(0, 'Entity', unc_df.index)
                unc_df = unc_df.reset_index(drop=True)
            else:
                unc_df = pd.DataFrame(
                    uncertainty,
                    index=entities,
                    columns=panel_data.components
                )
                unc_df.insert(0, 'Entity', unc_df.index)
                unc_df = unc_df.reset_index(drop=True)
            
            path = self.results_dir / f'prediction_uncertainty_{prediction_year}.csv'
            unc_df.to_csv(path, index=False, float_format='%.6f')
            saved_files['prediction_uncertainty'] = str(path)
        
        # 4. Save model contributions
        model_contributions = future_predictions.get('model_contributions', {})
        if model_contributions:
            contrib_df = pd.DataFrame([
                {'Model': k, 'Weight': v}
                for k, v in sorted(model_contributions.items(), 
                                   key=lambda x: x[1], reverse=True)
            ])
            path = self.results_dir / f'forecast_model_weights_{prediction_year}.csv'
            contrib_df.to_csv(path, index=False, float_format='%.6f')
            saved_files['model_weights'] = str(path)
        
        # 5. Save forecast summary
        summary = {
            'prediction_year': prediction_year,
            'training_years': future_predictions.get('training_years', []),
            'n_entities': len(entities),
            'n_components': len(panel_data.components),
            'top_predicted_entity': entities[np.argmax(to_array(future_predictions['topsis_scores']))],
            'top_predicted_score': float(np.max(to_array(future_predictions['topsis_scores']))),
            'mean_predicted_score': float(np.mean(to_array(future_predictions['topsis_scores']))),
            'model_contributions': model_contributions
        }
        
        path = self.results_dir / f'forecast_summary_{prediction_year}.json'
        with open(path, 'w') as f:
            json.dump(summary, f, indent=2, default=str)
        saved_files['forecast_summary'] = str(path)
        
        return saved_files
    
    def save_config_snapshot(self, config: Any) -> str:
        """
        Save configuration snapshot used for the analysis run.
        
        Parameters
        ----------
        config : Config
            Configuration object used for the analysis
        
        Returns
        -------
        str
            Path to saved config file
        """
        config_data = {
            'timestamp': datetime.now().isoformat(),
            'config': {}
        }
        
        # Extract config as dictionary if possible
        if hasattr(config, 'to_dict'):
            config_data['config'] = config.to_dict()
        elif hasattr(config, '__dict__'):
            # Convert dataclass-like objects
            def serialize(obj):
                if hasattr(obj, '__dict__'):
                    return {k: serialize(v) for k, v in obj.__dict__.items() 
                            if not k.startswith('_')}
                elif hasattr(obj, 'value'):  # Enum
                    return obj.value
                elif isinstance(obj, Path):
                    return str(obj)
                elif isinstance(obj, (list, tuple)):
                    return [serialize(i) for i in obj]
                elif isinstance(obj, dict):
                    return {k: serialize(v) for k, v in obj.items()}
                return obj
            config_data['config'] = serialize(config)
        
        path = self.results_dir / 'config_snapshot.json'
        with open(path, 'w') as f:
            json.dump(config_data, f, indent=2, default=str)
        
        return str(path)
    
    def save_figure_manifest(self, figure_paths: List[str]) -> str:
        """
        Save manifest of all generated figures with descriptions.
        
        Parameters
        ----------
        figure_paths : List[str]
            List of paths to generated figures
        
        Returns
        -------
        str
            Path to saved manifest
        """
        # Figure descriptions based on naming convention
        # NOTE: This project uses Random Forest (RF) for time-series forecasting
        figure_descriptions = {
            # Core MCDM Analysis (01-15)
            '01_score_evolution_top': 'Score evolution over time for top 10 performers',
            '02_score_evolution_bottom': 'Score evolution over time for bottom 10 performers',
            '03_weights_comparison': 'Comparison of criteria weights (Entropy vs CRITIC vs PCA vs Ensemble)',
            '04_topsis_scores': 'TOPSIS scores bar chart - complete ranking',
            '05_vikor_analysis': 'VIKOR analysis showing Q, S, R values',
            '06_method_agreement': 'MCDM methods ranking agreement (Spearman correlation matrix)',
            '07_score_distribution': 'TOPSIS score distribution histogram',
            '08_feature_importance': 'Random Forest feature importance for MCDM score prediction',
            '08_sigma_convergence': 'Sigma convergence analysis over time',
            '09_sensitivity_analysis': 'Criteria weight sensitivity analysis',
            '09_beta_convergence': 'Beta convergence analysis information',
            '10_feature_importance': 'Random Forest feature importance',
            '10_final_ranking': 'Final aggregated ranking summary',
            '11_sensitivity_analysis': 'Criteria weight sensitivity analysis',
            '11_method_comparison': 'MCDM methods ranking comparison (parallel coordinates)',
            '12_final_ranking': 'Final aggregated ranking summary',
            '12_ensemble_weights': 'Stacking ensemble model weights',
            '13_method_comparison': 'MCDM methods ranking comparison (parallel coordinates)',
            '13_future_predictions': 'Future predictions comparison (current vs predicted rankings)',
            '14_ensemble_weights': 'Stacking ensemble model weights',
            '15_future_predictions': 'Future predictions comparison (current vs predicted rankings)',
            
            # ML Process & Progress Visualizations (16-22) - Random Forest Based
            '16_rf_feature_importance_detailed': 'Random Forest feature importance with cumulative contribution analysis',
            '17_rf_cv_progression': 'Random Forest cross-validation score progression across folds',
            '18_rf_actual_vs_predicted': 'Random Forest model actual vs predicted values with regression analysis',
            '19_rf_residual_analysis': 'Random Forest model residual distribution with statistical bands',
            '20_rf_rank_correlation': 'Random Forest rank prediction accuracy with Spearman correlation',
            '21_ensemble_contribution': 'Ensemble model base model contribution with R² comparison',
            '22_rf_model_performance': 'Random Forest model performance metrics summary',
            
            # Legacy figure names (for backwards compatibility)
            'rf_analysis': 'Random Forest analysis (feature importance, CV scores, predictions)',
            'cv_results': 'Cross-validation performance results',
            'rf_prediction_analysis': 'Random Forest prediction vs actual analysis',
            'model_comparison': 'ML model performance comparison',
            'ensemble_model_analysis': 'Stacking ensemble analysis',
            'ml_dashboard': 'Comprehensive ML-MCDM analysis dashboard'
        }
        
        manifest = {
            'timestamp': datetime.now().isoformat(),
            'total_figures': len(figure_paths),
            'output_directory': str(self.figures_dir),
            'figures': []
        }
        
        for path in figure_paths:
            path_obj = Path(path)
            filename = path_obj.stem
            
            # Find matching description
            description = 'No description available'
            for key, desc in figure_descriptions.items():
                if key in filename:
                    description = desc
                    break
            
            manifest['figures'].append({
                'filename': path_obj.name,
                'path': str(path),
                'description': description,
                'format': path_obj.suffix.lstrip('.').upper()
            })
        
        path = self.figures_dir / 'figure_manifest.json'
        with open(path, 'w') as f:
            json.dump(manifest, f, indent=2)
        
        return str(path)
    
    def save_execution_summary(self, 
                               execution_time: float,
                               phase_timings: Optional[Dict[str, float]] = None) -> str:
        """
        Save machine-readable execution summary.
        
        Parameters
        ----------
        execution_time : float
            Total execution time in seconds
        phase_timings : Dict[str, float], optional
            Timing for each pipeline phase
        
        Returns
        -------
        str
            Path to saved summary
        """
        summary = {
            'timestamp': datetime.now().isoformat(),
            'total_execution_time_seconds': execution_time,
            'total_execution_time_formatted': f"{execution_time:.2f}s ({execution_time/60:.2f}min)",
            'phase_timings': phase_timings or {},
            'output_directories': {
                'results': str(self.results_dir),
                'figures': str(self.figures_dir),
                'reports': str(self.reports_dir)
            }
        }
        
        path = self.results_dir / 'execution_summary.json'
        with open(path, 'w') as f:
            json.dump(summary, f, indent=2)
        
        return str(path)
    
    def save_all_results(self,
                         panel_data: Any,
                         weights: Dict[str, np.ndarray],
                         mcdm_results: Dict[str, Any],
                         ml_results: Dict[str, Any],
                         ensemble_results: Dict[str, Any],
                         analysis_results: Dict[str, Any],
                         execution_time: float,
                         future_predictions: Optional[Dict[str, Any]] = None,
                         config: Optional[Any] = None,
                         figure_paths: Optional[List[str]] = None) -> Dict[str, Any]:
        """
        Save all analysis results in organized structure with robust error handling.
        
        Returns
        -------
        Dict[str, Any]
            Summary of all saved files
        """
        saved_files = {
            'timestamp': datetime.now().isoformat(),
            'execution_time': execution_time,
            'files': {}
        }
        
        # Helper function for safe saving
        def safe_save(name: str, save_func, *args, **kwargs):
            try:
                result = save_func(*args, **kwargs)
                saved_files['files'][name] = result
                return result
            except Exception as e:
                saved_files['files'][name] = {'error': str(e)}
                return None
        
        # Save all numerical results with error handling
        safe_save('weights', self.save_weights, weights, panel_data.components)
        
        safe_save('rankings', self.save_rankings, panel_data, mcdm_results, ensemble_results)
        
        safe_save('mcdm_scores', self.save_mcdm_scores, panel_data, mcdm_results)
        
        safe_save('mcdm_rank_comparison', self.save_mcdm_rank_comparison, panel_data, mcdm_results)
        
        # ML results (may be partially empty)
        if ml_results:
            safe_save('ml_results', self.save_ml_results, ml_results, panel_data)
        else:
            saved_files['files']['ml_results'] = {'status': 'skipped - no ML results'}
        
        # Ensemble results
        if ensemble_results:
            safe_save('ensemble_results', self.save_ensemble_results, ensemble_results, panel_data)
        else:
            saved_files['files']['ensemble_results'] = {'status': 'skipped - no ensemble results'}
        
        # Analysis results
        if analysis_results:
            safe_save('analysis_results', self.save_analysis_results, analysis_results)
        else:
            saved_files['files']['analysis_results'] = {'status': 'skipped - no analysis results'}
        
        # Future predictions (2025)
        if future_predictions:
            safe_save('future_predictions', self.save_future_predictions, 
                      future_predictions, panel_data)
        else:
            saved_files['files']['future_predictions'] = {'status': 'skipped - no future predictions'}
        
        safe_save('data_summary', self.save_panel_data_summary, panel_data)
        
        safe_save('report', self.generate_comprehensive_report,
                  panel_data, weights, mcdm_results, ml_results,
                  ensemble_results, analysis_results, execution_time,
                  future_predictions)
        
        # Save config snapshot if provided
        if config is not None:
            safe_save('config_snapshot', self.save_config_snapshot, config)
        
        # Save figure manifest if figure paths provided
        if figure_paths:
            safe_save('figure_manifest', self.save_figure_manifest, figure_paths)
        
        # Save execution summary
        safe_save('execution_summary', self.save_execution_summary, execution_time)
        
        # Save manifest
        try:
            manifest_path = self.results_dir / 'output_manifest.json'
            with open(manifest_path, 'w') as f:
                json.dump(saved_files, f, indent=2, default=str)
        except Exception as e:
            saved_files['manifest_error'] = str(e)
        
        return saved_files


def create_output_manager(output_dir: str = 'outputs') -> OutputManager:
    """Factory function to create output manager."""
    return OutputManager(output_dir)
